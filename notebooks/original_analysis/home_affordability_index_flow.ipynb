{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJ-2hoNWkZi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJWkUK3HU_K5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import plotly.express as px\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kxyd4ZYWvgt"
      },
      "source": [
        "# API Imports\n",
        "\n",
        "### Looking at the census data one comes to a big assumption that the data is very clean. Aquiring the dataset via the URL thoughwas found to be difficult. Different sections of the census repository were stored by different means. While some were in folders. This dataset was in a dashboard sysytem that supported API queries. Not only did this make the data collection easier but it also allowed for a skill that isnt used often to be utilied. I normally use Pandas Read function for all of my queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rn_oFIIWo_t"
      },
      "outputs": [],
      "source": [
        "def fetch_median_income_by_county(api_key, years):\n",
        "    all_years_data = []\n",
        "# This for loop takes an API key and queries the US Census database for the required demographic data. The URLs are standardized so its possible to query then in a loop by year.\n",
        "\n",
        "# Loop through each year and request data from the Census ACS 5-Year API\n",
        "    for year in years:\n",
        "        url = f'https://api.census.gov/data/{year}/acs/acs5'\n",
        "        params = {\n",
        "            'get': 'NAME,B19013_001E',\n",
        "            'for': 'county:*',\n",
        "            'key': api_key\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "# Their were many features in this dataset but the following were the only ones required for the purpose of this query.\n",
        "\n",
        "# Create DataFrame from API response\n",
        "        df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "# Rename columns for clarity\n",
        "        df = df.rename(columns={\n",
        "            'NAME': 'County',\n",
        "            'B19013_001E': 'Median_Household_Income',\n",
        "            'state': 'State_Code',\n",
        "            'county': 'County_Code'\n",
        "        })\n",
        "# An additional column was added using the year of query to organize each dataset prior to concatination.\n",
        "        df['Year'] = year\n",
        "        all_years_data.append(df)\n",
        "    df_all_years = pd.concat(all_years_data, ignore_index=True)\n",
        "# Convert income to numeric, coercing non-numeric entries to NaN\n",
        "    df_all_years['Median_Household_Income'] = pd.to_numeric(df_all_years['Median_Household_Income'], errors='coerce')\n",
        "# FIPS were not specifically available, state and county codes were merged to create the required FIPS code. This will be required to join other datasets to this one.\n",
        "    df_all_years['FIPS'] = df_all_years['State_Code'] + df_all_years['County_Code']\n",
        "    all_years_data_income = df_all_years\n",
        "    return all_years_data_income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQLw4Ds5Wul6"
      },
      "outputs": [],
      "source": [
        "all_years_data_income = fetch_median_income_by_county('92d47be7e95136939cd750005e2649fd073627e6', ['2012','2013','2014','2015','2016','2017','2018', '2019', '2020', '2021', '2022', '2023'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Go7FeNXSkP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptPJwM-1XapL"
      },
      "source": [
        "# Borrowed Nathan RedFin Import and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW5g7NhoXhzg"
      },
      "outputs": [],
      "source": [
        "df_housing_market = pd.read_csv('https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/county_market_tracker.tsv000.gz', sep='\\t')\n",
        "df_inflation = pd.read_csv('https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23ebf3fb&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1320&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=CPIAUCSL&scale=left&cosd=1947-01-01&coed=2025-04-01&line_color=%230073e6&link_values=false&line_style=solid&mark_type=none&mw=3&lw=3&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2025-06-06&revision_date=2025-06-06&nd=1947-01-01')\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        df_county_FIPS = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/national_county2020.txt', sep='|')\n",
        "        break\n",
        "    except Exception as e:\n",
        "        time.sleep(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6Daa3tFXlZ_"
      },
      "outputs": [],
      "source": [
        "# Combine state abbv and county name to match format found in Redfin housing market data.\n",
        "df_county_FIPS['county_and_state'] = [f\"{row[1][4]}, {row[1][0]}\" for row in df_county_FIPS.iterrows()]\n",
        "\n",
        "# Concat state FIPS and county FIPS, add leading zeros up to 5 digits.\n",
        "df_county_FIPS['county_and_state_FIPS'] = df_county_FIPS.STATEFP*1000+df_county_FIPS.COUNTYFP\n",
        "df_county_FIPS['county_and_state_FIPS'] = df_county_FIPS['county_and_state_FIPS'].astype(str).str.zfill(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HHe8KjjY_U6"
      },
      "outputs": [],
      "source": [
        "df_housing_market['REGION_lower'] = df_housing_market.REGION.str.lower()\n",
        "df_county_FIPS['county_and_state_lower'] = df_county_FIPS.county_and_state.str.lower()\n",
        "\n",
        "# Helper function for aligning Redfin county names to census county names.\n",
        "def county_name_matching(county):\n",
        "  add_city_list = [\n",
        "      'charlottesville', 'lynchburg', 'norfolk', 'portsmouth', 'harrisonburg',\n",
        "      'alexandria', 'virginia beach', 'manassas', 'manassas park',\n",
        "      'chesapeake', 'falls church', 'suffolk', 'hampton', 'fredericksburg',\n",
        "      'newport news', 'staunton', 'winchester', 'salem', 'petersburg',\n",
        "      'waynesboro', 'bristol', 'williamsburg', 'hopewell', 'danville',\n",
        "      'colonial heights', 'poquoson', 'martinsville', 'radford', 'buena vista',\n",
        "      'lexington', 'covington', 'norton', 'galax', 'emporia'\n",
        "  ]\n",
        "  add_city_and_list = [\n",
        "      'juneau borough', 'sitka borough', 'wrangell borough', 'yakutat borough'\n",
        "  ]\n",
        "  drop_county_list = [\n",
        "      'baltimore city county', 'richmond city county', 'roanoke city county',\n",
        "      'fairfax city county', 'st. louis city county', 'franklin city county'\n",
        "  ]\n",
        "  replace_borough_list = [\n",
        "      'anchorage borough', 'skagway borough'\n",
        "  ]\n",
        "  # If the Redfin county is in the add_city_list, add 'city' to it.\n",
        "  if county[:-4] in add_city_list:\n",
        "    return f'{county[:-4]} city{county[-4:]}'\n",
        "  # If the Redfin county is in the add_city_and_list, add 'city and' to it.\n",
        "  elif county[:-4] in add_city_and_list:\n",
        "    return f'{county[:-12]} city and{county[-12:]}'\n",
        "  # If the Redfin county is in the drop_county_list, drop 'county' from it.\n",
        "  elif county[:-4] in drop_county_list:\n",
        "    return f'{county[:-11]}{county[-4:]}'\n",
        "  # If the Redfin county is in replace_borough_list, change to 'municipality'.\n",
        "  elif county[:-4] in replace_borough_list:\n",
        "    return f'{county[:-12]} municipality{county[-4:]}'\n",
        "  # Replace '&' with 'and' for 'lewis & clark', and 'king & queen' counties.\n",
        "  elif '&' in county:\n",
        "    return county.replace('&', 'and')\n",
        "  # Remove space from 'la salle'.\n",
        "  elif county=='la salle parish, la':\n",
        "    return 'lasalle parish, la'\n",
        "  # If not in the above cases, return original.\n",
        "  else:\n",
        "    return county\n",
        "\n",
        "# Apply the helper function.\n",
        "df_housing_market['REGION_lower'] = df_housing_market['REGION_lower'].apply(\n",
        "    county_name_matching\n",
        "    )\n",
        "# Add FIPS codes to housing market data, on matching county names.\n",
        "df_housing_market = pd.merge(\n",
        "    df_housing_market,\n",
        "    df_county_FIPS[['county_and_state_lower', 'county_and_state_FIPS']],\n",
        "    left_on='REGION_lower',\n",
        "    right_on='county_and_state_lower',\n",
        "    how='left'\n",
        "    )\n",
        "# Two special cases not handled by helper function, add FIPS code directly.\n",
        "df_housing_market.loc[\n",
        "    df_housing_market['REGION_lower']==\n",
        "    'dona ana county, nm', 'county_and_state_FIPS'\n",
        "    ] = '35013'\n",
        "df_housing_market.loc[\n",
        "    df_housing_market['REGION_lower']==\n",
        "    'valdez-cordova census area, ak', 'county_and_state_FIPS'\n",
        "    ] = '02261'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR9gIkDsZb-2"
      },
      "outputs": [],
      "source": [
        "df_housing_market['PERIOD_BEGIN'] = pd.to_datetime(df_housing_market['PERIOD_BEGIN'])\n",
        "df_housing_market['PERIOD_END'] = pd.to_datetime(df_housing_market['PERIOD_END'])\n",
        "df_inflation['observation_date'] = pd.to_datetime(df_inflation['observation_date'])\n",
        "\n",
        "\n",
        "df_inflation = df_inflation.rename(columns={'CPIAUCSL': 'CPIAUCNS'})\n",
        "\n",
        "df_housing_market = pd.merge(\n",
        "    df_housing_market,\n",
        "    df_inflation,\n",
        "    left_on='PERIOD_BEGIN',\n",
        "    right_on='observation_date',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df_housing_market.drop('observation_date', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1f3XpEPZemO"
      },
      "outputs": [],
      "source": [
        "df_housing_market = df_housing_market.rename(columns={\n",
        "    'county_and_state_FIPS':'FIPS_CODE',\n",
        "    'CPIAUCNS':'CPI'\n",
        "    })\n",
        "\n",
        "df_housing_market = df_housing_market[[\n",
        "    'FIPS_CODE','REGION', 'PROPERTY_TYPE', 'PERIOD_BEGIN',\n",
        "    'MEDIAN_SALE_PRICE', 'MEDIAN_LIST_PRICE',\n",
        "    'CPI', 'INVENTORY', 'HOMES_SOLD'\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRclPJjqZhET"
      },
      "outputs": [],
      "source": [
        "missing_counts = df_housing_market[df_housing_market['MEDIAN_SALE_PRICE'].isna()]['FIPS_CODE'].value_counts().rename('Missing_Count')\n",
        "total_counts = df_housing_market['FIPS_CODE'].value_counts().rename('Total_Count')\n",
        "df_missing_sales = pd.merge(\n",
        "    missing_counts,\n",
        "    total_counts,\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='left'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsvhakj9Zjv-"
      },
      "outputs": [],
      "source": [
        "df_housing_market = df_housing_market.sort_values(by=['FIPS_CODE', 'PROPERTY_TYPE', 'PERIOD_BEGIN'])\n",
        "df_housing_market = df_housing_market.reset_index().drop(columns=['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awb-ATaWZmlS"
      },
      "outputs": [],
      "source": [
        "def add_zscore_outlier_cols(df, col):\n",
        "  # Add columns to track z-score and if a value is an outlier.\n",
        "  is_outlier_col = col+'_is_outlier'\n",
        "  zscore_col = col+'_zscore'\n",
        "  df[zscore_col] = df.groupby(['FIPS_CODE', 'PROPERTY_TYPE'])[[col]].transform(stats.zscore)\n",
        "  # Define an outlier threshold here.\n",
        "  max_z = 3\n",
        "  df[is_outlier_col] = abs(df[zscore_col]>max_z)\n",
        "  return df\n",
        "\n",
        "# Add z-score and outlier columns.\n",
        "price_cols = ['MEDIAN_SALE_PRICE', 'MEDIAN_LIST_PRICE']\n",
        "for price_col in price_cols:\n",
        "  df_housing_market = add_zscore_outlier_cols(df_housing_market, price_col)\n",
        "\n",
        "# Drop z-score columns.\n",
        "price_cols_zscore = [price_col+'_zscore' for price_col in price_cols]\n",
        "df_housing_market = df_housing_market.drop(columns=price_cols_zscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtCfwMIzaBhO"
      },
      "outputs": [],
      "source": [
        "def outliers_to_nan(df, col):\n",
        "  # Add new column that removes values if they are tagged as outliers.\n",
        "  is_outlier_col = col+'_is_outlier'\n",
        "  no_outliers_col = col+'_no_outliers'\n",
        "  df[no_outliers_col] = df[col]\n",
        "  df.loc[df[is_outlier_col], no_outliers_col] = np.nan\n",
        "  return df\n",
        "\n",
        "# Change the price column value to NaN for outliers.\n",
        "for price_col in price_cols:\n",
        "  df_housing_market = outliers_to_nan(df_housing_market, price_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkR1xeiHaFDb"
      },
      "outputs": [],
      "source": [
        "# Series for every unique FIPS in the dataset.\n",
        "all_FIPS = pd.Series(df_housing_market['FIPS_CODE'].unique()).rename('FIPS_CODE')\n",
        "# Series for every unique property type in the dataset.\n",
        "all_types = pd.Series(df_housing_market['PROPERTY_TYPE'].unique()).rename('PROPERTY_TYPE')\n",
        "# Series for every month between min and max values of PERIOD_BEGIN.\n",
        "all_months = pd.Series(\n",
        "    pd.date_range(\n",
        "        start=df_housing_market['PERIOD_BEGIN'].min(),\n",
        "        end=df_housing_market['PERIOD_BEGIN'].max(),\n",
        "        freq='MS'# Each month, starting date.\n",
        "        )\n",
        "    ).rename('PERIOD_BEGIN')\n",
        "\n",
        "# Cross join the three series above so every FIPS/property type group\n",
        "# has rows for every month.\n",
        "df_all_groups_all_months = pd.merge(\n",
        "    all_FIPS,\n",
        "    all_types,\n",
        "    how='cross'\n",
        "    ).merge(\n",
        "        all_months,\n",
        "        how='cross'\n",
        "    )\n",
        "\n",
        "# Before merging our housing market data to many new rows,\n",
        "# denote current rows as original records.\n",
        "df_housing_market['original_record'] = True\n",
        "\n",
        "# Merge our housing market data onto the list of every month for every group.\n",
        "df_housing_market = pd.merge(\n",
        "    df_all_groups_all_months,\n",
        "    df_housing_market,\n",
        "    on=['FIPS_CODE', 'PROPERTY_TYPE', 'PERIOD_BEGIN'],\n",
        "    how='left'\n",
        "    )\n",
        "\n",
        "# Fill in region and CPI columns.\n",
        "# For each FIPS, there is only one region value.\n",
        "df_housing_market['REGION'] =\\\n",
        "  df_housing_market.groupby('FIPS_CODE')['REGION'].ffill()\n",
        "df_housing_market['REGION'] =\\\n",
        "  df_housing_market.groupby('FIPS_CODE')['REGION'].bfill()\n",
        "# For each month, theres is only one CPI value.\n",
        "df_housing_market['CPI'] =\\\n",
        "  df_housing_market.groupby('PERIOD_BEGIN')['CPI'].ffill()\n",
        "df_housing_market['CPI'] =\\\n",
        "  df_housing_market.groupby('PERIOD_BEGIN')['CPI'].bfill()\n",
        "\n",
        "# Fill NaN's in \"original_record\" column, and \"_is_outlier\" columns with False.\n",
        "fill_values = {\n",
        "    'original_record': False,\n",
        "    'MEDIAN_SALE_PRICE_is_outlier': False,\n",
        "    'MEDIAN_LIST_PRICE_is_outlier': False\n",
        "    }\n",
        "df_housing_market = df_housing_market.fillna(fill_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBSpo7CiaH9t"
      },
      "outputs": [],
      "source": [
        "def interpolate_col(df, col):\n",
        "  # Create a column to receive interpolated values.\n",
        "  interpolated_col = col+'_interpolated'\n",
        "  df[interpolated_col] = np.nan\n",
        "  # Interpolation will be performed on the column without outliers.\n",
        "  no_outliers_col = col+'_no_outliers'\n",
        "  # Create a column to note these values are interpolated.\n",
        "  is_interpolated_col = col+'_is_interpolated'\n",
        "  df[is_interpolated_col] = df[no_outliers_col].isna()\n",
        "\n",
        "  # From: https://stackoverflow.com/questions/37057187/pandas-interpolate-within-a-groupby\n",
        "  # Define a new df to perform interpolation on.\n",
        "  df_interpolated = df[['FIPS_CODE', 'PROPERTY_TYPE', 'PERIOD_BEGIN', no_outliers_col]].set_index('PERIOD_BEGIN')\n",
        "  # Group by FIPS and property type, interpolate for months missing data.\n",
        "  df_interpolated = df_interpolated.groupby(['FIPS_CODE', 'PROPERTY_TYPE'])\\\n",
        "   [['FIPS_CODE', 'PROPERTY_TYPE', no_outliers_col]].apply(\n",
        "       lambda group: group.interpolate(\n",
        "           method='time',\n",
        "           )\n",
        "       )\n",
        "  # Return our original input df, with a new column of interpolated values.\n",
        "  df[interpolated_col] = df_interpolated[no_outliers_col].values\n",
        "  return df\n",
        "\n",
        "# Add interpolated columns.\n",
        "price_cols = ['MEDIAN_SALE_PRICE', 'MEDIAN_LIST_PRICE']\n",
        "for col in price_cols:\n",
        "  df_housing_market = interpolate_col(df_housing_market, col)\n",
        "\n",
        "# Fill in inventory and homes sold columns.\n",
        "# For inventory, we assume if there is no reported sales,\n",
        "# then inventory is constant from last reported sales record.\n",
        "df_housing_market['INVENTORY'] = \\\n",
        "  df_housing_market.groupby(['FIPS_CODE','PROPERTY_TYPE'])\\\n",
        "  ['INVENTORY'].ffill()\n",
        "# For homes sold column, we assume if there is no reported sales, 0 homes sold.\n",
        "df_housing_market = df_housing_market.fillna({'HOMES_SOLD': 0})\n",
        "\n",
        "# The above fill methods and interpolation will not cover\n",
        "# groups with earliest months missing (primarily in 2012),\n",
        "# for these there are limited options, we will backfill.\n",
        "df_housing_market['MEDIAN_SALE_PRICE_interpolated'] = \\\n",
        "  df_housing_market.groupby(['FIPS_CODE','PROPERTY_TYPE'])\\\n",
        "  ['MEDIAN_SALE_PRICE_interpolated'].bfill()\n",
        "df_housing_market['MEDIAN_LIST_PRICE_interpolated'] = \\\n",
        "  df_housing_market.groupby(['FIPS_CODE','PROPERTY_TYPE'])\\\n",
        "  ['MEDIAN_LIST_PRICE_interpolated'].bfill()\n",
        "df_housing_market['INVENTORY'] = \\\n",
        "  df_housing_market.groupby(['FIPS_CODE','PROPERTY_TYPE'])\\\n",
        "  ['INVENTORY'].bfill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwwr_zSxaKvO"
      },
      "outputs": [],
      "source": [
        "def inflation_adjustment(df, price_col):\n",
        "  # Get our earliest record, and the CPI on that date.\n",
        "  earliest_period = min(df['PERIOD_BEGIN'])\n",
        "  CPI_init = df[df_housing_market['PERIOD_BEGIN']==earliest_period]['CPI'][0]\n",
        "\n",
        "  # Create an inflation adjusted column from the price column.\n",
        "  inflation_col = f'{price_col}_inflation_adj_{str(earliest_period)[0:7]}'\n",
        "  inflation_adjustment_col = df['CPI']/CPI_init\n",
        "  df[inflation_col] = df[price_col]*inflation_adjustment_col\n",
        "  return df\n",
        "\n",
        "# Add inflation adjusted columns.\n",
        "price_cols = [\n",
        "    'MEDIAN_SALE_PRICE', 'MEDIAN_SALE_PRICE_no_outliers',\n",
        "    'MEDIAN_SALE_PRICE_interpolated',\n",
        "    'MEDIAN_LIST_PRICE', 'MEDIAN_LIST_PRICE_no_outliers',\n",
        "    'MEDIAN_LIST_PRICE_interpolated'\n",
        "    ]\n",
        "for price_col in price_cols:\n",
        "  df_housing_market = inflation_adjustment(df_housing_market, price_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIvzRmEYaeiR"
      },
      "outputs": [],
      "source": [
        "df_housing_market_updated = df_housing_market\n",
        "df_housing_market_updated['Year'] = pd.to_datetime(df_housing_market_updated['PERIOD_BEGIN']).dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "AcNLewb4anmo",
        "outputId": "8f3ff163-7170-4283-e73b-8b3fbdd45bb2"
      },
      "outputs": [],
      "source": [
        "df_housing_market_grouped = df_housing_market_updated.groupby(['FIPS_CODE', 'Year']).agg({\n",
        "    'MEDIAN_SALE_PRICE': 'median',\n",
        "    'MEDIAN_LIST_PRICE': 'median'\n",
        "}).reset_index()\n",
        "\n",
        "df_housing_market_grouped = df_housing_market_grouped.rename(columns={'FIPS_CODE': 'FIPS'})\n",
        "\n",
        "df_housing_market_grouped.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leAMUdkVa28h"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1vKut5xa_Ie"
      },
      "source": [
        "# Calculate Affordability Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvezTyagIKoN"
      },
      "source": [
        "### Initially home affordability index was assumed to be Income-to-Home Value Ratio(median home value / median Income). After further research that was the wrong metric. home affordability index (median income / required income to afford a median priced home). This required estimation in values such at  20% down payment (loan_to_value), mortgage term period, and the gross monthly income that can be used for monthly payments (housing_ratio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNxD15Vca9WX"
      },
      "outputs": [],
      "source": [
        "def calculate_home_affordability_index(income_df, housing_df,\n",
        "                                       loan_to_value=0.8,\n",
        "                                       loan_term_years=30,\n",
        "                                       annual_interest_rate=0.05,\n",
        "                                       housing_ratio=0.28):\n",
        "# Ensure Year is integer\n",
        "    income_df['Year'] = income_df['Year'].astype(int)\n",
        "    housing_df['Year'] = housing_df['Year'].astype(int)\n",
        "\n",
        "# Merge datasets\n",
        "    merged_df = pd.merge(\n",
        "        income_df[['FIPS', 'Year', 'Median_Household_Income']],\n",
        "        housing_df[['FIPS', 'Year', 'MEDIAN_SALE_PRICE', 'MEDIAN_LIST_PRICE']],\n",
        "        on=['FIPS', 'Year'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "# Loan assumptions\n",
        "    loan_term_months = loan_term_years * 12\n",
        "    monthly_interest_rate = annual_interest_rate / 12\n",
        "\n",
        "# Estimate mortgage and affordability\n",
        "    merged_df['Loan_Amount'] = merged_df['MEDIAN_SALE_PRICE'] * loan_to_value\n",
        "    merged_df['Monthly_Payment'] = (\n",
        "        merged_df['Loan_Amount'] * monthly_interest_rate * (1 + monthly_interest_rate) ** loan_term_months\n",
        "    ) / ((1 + monthly_interest_rate) ** loan_term_months - 1)\n",
        "\n",
        "    merged_df['Required_Income'] = (merged_df['Monthly_Payment'] * 12) / housing_ratio\n",
        "\n",
        "    merged_df['HAI'] = np.where(\n",
        "        merged_df[['Median_Household_Income', 'Required_Income']].notna().all(axis=1),\n",
        "        (merged_df['Median_Household_Income'] / merged_df['Required_Income']) * 100,\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    merged_df_no_na = merged_df.dropna(subset=['HAI'])\n",
        "    return merged_df_no_na"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70N0vVG1bNW8"
      },
      "outputs": [],
      "source": [
        "merged_df_no_na = calculate_home_affordability_index(\n",
        "    all_years_data_income,\n",
        "    df_housing_market_grouped\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H-0TR2QAbwwq",
        "outputId": "b6e2ccc6-1aea-49fd-b058-98f2ce7ac77a"
      },
      "outputs": [],
      "source": [
        "merged_df_no_na.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcSRXNTKcks9"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqULhTdH6hi"
      },
      "source": [
        "### Initially this data was visualized using Altair. Their was an issue with the visual not properly displaying the data for multiple regions.I then switched to Plotly and was able to properly visualize the dataset. The altair visual was off due to the data overriding itself when trying to add data slider. If I hardcoded the data, the visual representation was correct but not with the slider. I found plotly to providemore capability with its choropleth visual as well. I also added a trending plot to show the counties some major cities are in to provide additional insight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNdxIU0hb8vY"
      },
      "outputs": [],
      "source": [
        "def plot_hai_visualizations(df_input):\n",
        "# Load US counties GeoJSON\n",
        "    geojson_url = 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'\n",
        "    counties_geojson = requests.get(geojson_url).json()\n",
        "\n",
        "# Prepare data\n",
        "    df = df_input.copy()\n",
        "    df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "\n",
        "# Choropleth Map\n",
        "    fig_choropleth = px.choropleth(\n",
        "        df,\n",
        "        geojson=counties_geojson,\n",
        "        locations='FIPS',\n",
        "        color='HAI',\n",
        "        color_continuous_scale='Plasma',\n",
        "        range_color=(0, 200),\n",
        "        scope='usa',\n",
        "        labels={'HAI': 'Affordability Index'},\n",
        "        animation_frame='Year',\n",
        "        title='Home Affordability Index (HAI) by U.S. County Over Time'\n",
        "    )\n",
        "\n",
        "    fig_choropleth.update_geos(fitbounds=\"locations\", visible=False)\n",
        "    fig_choropleth.update_layout(\n",
        "        margin={\"r\": 0, \"t\": 50, \"l\": 0, \"b\": 0},\n",
        "        width=1200,\n",
        "        height=800\n",
        "    )\n",
        "    fig_choropleth.show()\n",
        "\n",
        "# added line plot visual here with hardcoded FIPS\n",
        "    city_fips = {\n",
        "        '36061': 'New York City',\n",
        "        '12086': 'Miami',\n",
        "        '06037': 'Los Angeles',\n",
        "        '17031': 'Chicago'\n",
        "    }\n",
        "\n",
        "    df_selected = df[df['FIPS'].isin(city_fips.keys())].copy()\n",
        "    df_selected['City'] = df_selected['FIPS'].map(city_fips)\n",
        "\n",
        "    fig_line = px.line(\n",
        "        df_selected,\n",
        "        x='Year',\n",
        "        y='HAI',\n",
        "        color='City',\n",
        "        title='Home Affordability Index (HAI) Trend for Selected Cities',\n",
        "        markers=True\n",
        "    )\n",
        "\n",
        "# chose size layout for the visual\n",
        "    fig_line.update_layout(\n",
        "        xaxis_title='Year',\n",
        "        yaxis_title='Home Affordability Index (HAI)',\n",
        "        width=1000,\n",
        "        height=600\n",
        "    )\n",
        "    fig_line.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mwhyw8X5cG27",
        "outputId": "93c9f475-6ef5-4ae4-ac73-bcea4d9e945e"
      },
      "outputs": [],
      "source": [
        "plot_hai_visualizations(merged_df_no_na)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7s67neL8526"
      },
      "source": [
        "# Report Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNt4RyUOcM1Y"
      },
      "outputs": [],
      "source": [
        "fips = pd.read_csv('data/County_FIPS.txt', sep='|')\n",
        "\n",
        "fips['STATEFP'] = fips['STATEFP'].astype(str).str.zfill(2)\n",
        "fips['COUNTYFP'] = fips['COUNTYFP'].astype(str).str.zfill(3)\n",
        "fips['FIPS'] = fips['STATEFP'] + fips['COUNTYFP']\n",
        "\n",
        "fips['COUNTYSTATE'] = fips['COUNTYNAME'] + ', ' + fips['STATE']\n",
        "\n",
        "fips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JCxHvpg9AWr"
      },
      "outputs": [],
      "source": [
        "def plot_hai_visualizations_v2(df_input, fips_codes):\n",
        "# Load US counties GeoJSON\n",
        "    geojson_url = 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'\n",
        "    counties_geojson = requests.get(geojson_url).json()\n",
        "\n",
        "# Prepare data\n",
        "    df = df_input.copy()\n",
        "    df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "    df = pd.merge(df, fips_codes, on='FIPS', how='inner')\n",
        "\n",
        "    custom_diverging = [\n",
        "        \"#d95f02\",\n",
        "        \"#d3d3d3\",\n",
        "        \"#1f77b4\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    #Fixing the year to export static visuals\n",
        "    year = 2012\n",
        "    df_year = df[df['Year'] == year]\n",
        "\n",
        "    fig_choropleth = px.choropleth(\n",
        "        df_year,\n",
        "        geojson=counties_geojson,\n",
        "        locations='FIPS',\n",
        "        color='HAI',\n",
        "        color_continuous_scale=custom_diverging,\n",
        "        range_color=(0, 200),\n",
        "        scope='usa',\n",
        "        hover_data={'COUNTYNAME': True},\n",
        "        labels={'HAI': 'Affordability Index', 'COUNTYNAME': 'County Name'},\n",
        "        title=f'Home Affordability Index (HAI) by U.S. County - {year}'\n",
        "    )\n",
        "\n",
        "    fig_choropleth.update_geos(\n",
        "        visible=False,\n",
        "        projection_type=\"albers usa\"\n",
        "    )\n",
        "\n",
        "    fig_choropleth.update_layout(\n",
        "        margin=dict(l=0, r=0, t=30, b=0),\n",
        "        width=800,\n",
        "        height=500,\n",
        "        title_x=0.5,\n",
        "        title_y=0.95,\n",
        "        title_font=dict(size=16),\n",
        "        coloraxis_colorbar=dict(\n",
        "            thickness=20,\n",
        "            len=.7,\n",
        "            x=0.9,\n",
        "            y=0.5,\n",
        "            title=\"HAI\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig_choropleth.show()\n",
        "\n",
        "\n",
        "# added line plot visual here with hardcoded FIPS\n",
        "    city_fips = ['06037','17031','48201','04013','12086','36047','36081','06059']\n",
        "\n",
        "    df_selected = df[df['FIPS'].isin(city_fips)].copy()\n",
        "\n",
        "    fig_line = px.line(\n",
        "        df_selected,\n",
        "        x='Year',\n",
        "        y='HAI',\n",
        "        color='COUNTYSTATE',\n",
        "        title='Home Affordability Index (HAI) Trend for Largest Counties',\n",
        "        markers=True,\n",
        "        color_discrete_sequence=px.colors.qualitative.D3\n",
        "    )\n",
        "\n",
        "    fig_line.update_layout(\n",
        "        xaxis_title='Year',\n",
        "        yaxis_title='Home Affordability Index (HAI)',\n",
        "        width=1000,\n",
        "        height=600,\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='white',\n",
        "        font=dict(size=14),\n",
        "        legend_title_text='City',\n",
        "        title_font=dict(size=18)\n",
        "    )\n",
        "\n",
        "    fig_line.update_xaxes(showgrid=False)\n",
        "    fig_line.update_yaxes(showgrid=True, gridcolor='lightgrey')\n",
        "\n",
        "    fig_line.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2nGOQNC9HBG"
      },
      "outputs": [],
      "source": [
        "plot_hai_visualizations_v2(merged_df_no_na, fips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmPpzmaj9KH4"
      },
      "outputs": [],
      "source": [
        "def plot_hai_visualizations_animated_v2(df_input, fips_codes):\n",
        "# Load US counties GeoJSON\n",
        "    geojson_url = 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'\n",
        "    counties_geojson = requests.get(geojson_url).json()\n",
        "\n",
        "# Prepare data\n",
        "    df = df_input.copy()\n",
        "    df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n",
        "\n",
        "    df = pd.merge(df, fips_codes, on='FIPS', how='inner')\n",
        "\n",
        "    custom_diverging = [\n",
        "        \"#d95f02\",\n",
        "        \"#d3d3d3\",\n",
        "        \"#1f77b4\"\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "    fig_choropleth = px.choropleth(\n",
        "        df,\n",
        "        geojson=counties_geojson,\n",
        "        locations='FIPS',\n",
        "        color='HAI',\n",
        "        color_continuous_scale=custom_diverging,\n",
        "        range_color=(0, 200),\n",
        "        scope='usa',\n",
        "        hover_data={'COUNTYNAME': True},\n",
        "        labels={'HAI': 'Affordability Index', 'COUNTYNAME': 'County Name'},\n",
        "        animation_frame='Year',\n",
        "        title=f'Home Affordability Index (HAI) by U.S. County'\n",
        "    )\n",
        "\n",
        "    fig_choropleth.update_geos(\n",
        "        visible=False,\n",
        "        projection_type=\"albers usa\"\n",
        "    )\n",
        "\n",
        "    fig_choropleth.update_layout(\n",
        "        margin=dict(l=0, r=0, t=30, b=0),\n",
        "        width=800,\n",
        "        height=500,\n",
        "        title_x=0.5,\n",
        "        title_y=0.95,\n",
        "        title_font=dict(size=16),\n",
        "        coloraxis_colorbar=dict(\n",
        "            thickness=20,\n",
        "            len=.7,\n",
        "            x=0.9,\n",
        "            y=0.5,\n",
        "            title=\"HAI\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig_choropleth.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_izcpDV9Wep"
      },
      "outputs": [],
      "source": [
        "plot_hai_visualizations_animated_v2(merged_df_no_na, fips)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3eJ-2hoNWkZi",
        "4Kxyd4ZYWvgt",
        "ptPJwM-1XapL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
